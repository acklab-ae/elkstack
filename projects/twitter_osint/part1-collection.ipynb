{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and Process Twitter Feeds for Data Analysis: the Pythonic Way\n",
    "\n",
    "## OSINT with Python and ELKstack [Part 1]\n",
    "> Andrew Eng | 2020-10-05 | Part 1 \n",
    "\n",
    "Updated: 2020-10-09:\n",
    "- Cleaned up code blocks to make sure it runs without errors\n",
    "\n",
    "**This article was renamed from Open Source Intelligence with Elasticsearch: Analyzing Twitter Feeds 1/3 as the title no longer fit the description of the series.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is a three part series in open source intelligence with Elasticsearch.\n",
    "\n",
    "In part 1: We will look at data acquisition and building a python script to acquire, clean, and transfer data.\n",
    "\n",
    "In part 2: We will look at building visualizations and dashboards in Kibana and Elasticsearch.\n",
    "\n",
    "In Part 3: We will look at operationalizing the script into sustainment.\n",
    "\n",
    "## Start with Why\n",
    "I had an opportunity to interview with a large tech company but I didn’t know much about them (I’ve been oblivious to what’s popping up in Silicon Valley). Following due diligence and better prepare for the interview, I spent the weekend working on script to help me better understand the general sentiments and what the company was about. At that point in time, any news is new news, and I wanted to consume it.\n",
    "\n",
    "Twitter is a great social networking platform that delivers almost real-time news generated and posted by “regular people”. It’s great, because it’s really unfiltered and you’re able to somewhat grasp what people are thinking. **The goal of this project is to build a twitter scrapper, dump it into Elasticsearch and create visualization dashboards using Kibana**.\n",
    "\n",
    "Like most people, I’m a visual person. I love looking at dashboards, graphs, and pictures that makes sense of data. It helps me comprehend and gain insight in a particular subject. I’m also a project based learner. I learn better when I have an end goal. In this case, I’m learning: Python, Elasticsearch, and Kibana. I’m also learning how to use the Twitter API for other projects that I’m working on like Natural Language Processing (Sentiment Analysis, bots, docker, etc.)\n",
    "\n",
    "### Visualizations: What questions are we trying to answer?\n",
    "My main focus is to collect as much information about my search as possible. I’d like to track trending tweets, stories, and what people are generally thinking about the subject. With this, I can probably track “influencers. I’m interested in the social impact on how influencers influence. My initial questions to answer are:\n",
    "\n",
    "1. What are the most retweets of the specific search term? (Tracking topic popularity)\n",
    "2. How many unique users are tweeting about a given search term? (Is it trending?)\n",
    "3. What other tweets are they posting about the search term? (Are they a troll / bot?)\n",
    "4. How many likes does the person have on the search term? (Trending / Sentiments)?\n",
    "\n",
    "## My Environment\n",
    "It really doesn’t matter what I’m using, because the processing is low overhead. It’s not processing intensive calculations or stitching anything together. It’s simple: grab tweets, format it into JSON or dictionary, and send it off to Elasticsearch. For this environment, I’ll be using docker to quickly stand-up an Elasticsearch and Kibana (ELKSTACK) so I can focus on the coding parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker\n",
    "\n",
    "<code>$ git clone https://github.com/andreweng/elkstack.git</code>\n",
    "\n",
    "<code>$ cd elkstack</code>\n",
    "\n",
    "<code>$ cd docker-compose up</code>\n",
    "\n",
    "![docker-compose up](images/docker-compose_up.png)\n",
    "\n",
    "Afterwards, test you have ELKSTACK up and running by browsing to: **[http://localhost:5601](http://localhost:5601)**\n",
    "\n",
    "![initial_kibana](images/initial_kibana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "For my twitter scrapper, I’ll be using python 3.8.3 and my IDE will be Jupyter Notebook. Jupyter Notebook is a great Integrated Development Environment that allows you to write code in blocks and run it individually within a web browser. It’s easy to initiate and secure enough to get the job done and tear it down. By default, Jupyter will create open port 8888 on localhost (127.0.0.1) with unique tokens every time you initiate it.\n",
    "\n",
    "<code>$ cd ~/elkstack/scripts</code>\n",
    "\n",
    "<code>$ jupyter-notebook</code>\n",
    "\n",
    "![jupyter_initiate](images/jupyter_initiate.png)\n",
    "\n",
    "The command above is run on a terminal window while inside the elkstack/scripts folder.\n",
    "\n",
    "## Coding\n",
    "### Twitter API\n",
    "Since we are going to programmatically access twitter and pull records, we’ll need to use the Twitter API. You can grab your APIs following the Twitter API link [https://developer.twitter.com/en](https://developer.twitter.com/en)\n",
    "\n",
    "#### Authentication\n",
    "There are 3 types of authentication mechanisms we can use, however basic authentication is deprecated for awhile now:\n",
    "\n",
    "- **OAuth 1.0a is a method used to make API requests on behalf of a Twitter Account. In other words, application-user authentication.**\n",
    "- OAuth 2.0 Bearer Token is a authenticates requests on behalf of your developer App. Specifically, it allows your app to pull records in READ-ONLY. In other words, application authentication.\n",
    "- Basic Authentication accesses twitter as you, it’s basic authentication where you pass your email and password to the app [no longer in use].\n",
    "\n",
    "For this use-case, I’ll be using the **OAuth 1.0a**, this is mainly so I can access a full range of APIs for later scalability.\n",
    "\n",
    "![Twitter_Auth_API](images/twitter-auth-api.png)\n",
    "\n",
    "### Python Modules\n",
    "\n",
    "**Quick install of modules:**\n",
    "\n",
    "<code>$ sudo pip3 install -r requirements.txt</code>\n",
    "\n",
    "Let's take a look at what modules we are using.  If you used requirements.txt to install the modules, you don't need to run the below commands.\n",
    "\n",
    "**External modules:**\n",
    "\n",
    "[Tweepy](https://github.com/tweepy/tweepy) **|** [Documentation](http://docs.tweepy.org/en/latest/index.html)  \n",
    "\n",
    "I created a notebook that I used to take notes and play around with the module [Tweepy_tutorial](tweepy_tutorial.ipy)\n",
    "\n",
    "[Elasticsearch](https://github.com/elastic/elasticsearch-py) **|** [Documentation](https://elasticsearch-py.readthedocs.io/en/master/)\n",
    "\n",
    "**Built in modules:**\n",
    "\n",
    "[json Documentation](https://docs.python.org/3/library/json.html)\n",
    "\n",
    "[datetime Documentation](https://docs.python.org/3/library/datetime.html)\n",
    "\n",
    "<code>$ sudo pip3 install tweepy elasticsearch</code>\n",
    "\n",
    "![pip3_install_tweepy](images/tweepy_install.png)\n",
    "![pip3_install_elasticsearch](images/elasticsearch_install.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "Import the modules we be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "from datetime import datetime as dt\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it's not a good idea to include credentials and access tokens in scripts.  There's additional security stuff we can explore when we operationalize the script, but for now, let's get this up and running.  Create your API credentials text file and name it twitter.keys. My twitter.keys file looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "(base) andrew@tangent:~$ cat twitter.keys \n",
    "api_key = <redacted>\n",
    "api_secret_key = <redacted>\n",
    "access_token = <redacted>\n",
    "access_token_secret = <redacted>\n",
    "(base) andrew@tangent:~$ \n",
    "```\n",
    "\n",
    "Import keys into the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keys from a saved file instead of inputting it directly into the script\n",
    "key_location = 'twitter.keys'\n",
    "apikeys = []\n",
    "with open(key_location) as keys:\n",
    "    for items in keys:\n",
    "        apikeys.append(items.split('=')[1].strip(' ').strip('\\n'))\n",
    "    keys.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the key information to the script and set server configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary\n",
    "twitter_cred = dict()\n",
    "\n",
    "# Enter API keys\n",
    "twitter_cred[\"CONSUMER_KEY\"] = apikeys[0]\n",
    "twitter_cred[\"CONSUMER_SECRET\"] = apikeys[1]\n",
    "\n",
    "# Access Tokens\n",
    "twitter_cred[\"ACCESS_KEY\"] = apikeys[2]\n",
    "twitter_cred[\"ACCESS_SECRET\"] = apikeys[3]\n",
    "\n",
    "auth = tw.OAuthHandler(twitter_cred[\"CONSUMER_KEY\"], twitter_cred[\"CONSUMER_SECRET\"])\n",
    "\n",
    "auth.set_access_token(twitter_cred[\"ACCESS_KEY\"], twitter_cred[\"ACCESS_SECRET\"])\n",
    "\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Initialize elasticsearch node\n",
    "es = Elasticsearch('127.0.0.1', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hello World!** Let’s test our API to see if our authentication is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times @nytimes: Facebook and the Group That Planned to Kidnap Gretchen Whitmer. #dataresponsible #AI… https://t.co/rM9pYLyxJP\n",
      "The $25M class action against Tezos was a 'lawyers' picnic' ... that has now descended into an unseemly brawl over… https://t.co/7Klirw1Tvf\n",
      "Artificial Intelligence research at University of Adelaide gets $20 million boost. #dataresponsible #robotics… https://t.co/IM6ZwyHQUW\n",
      "Abuse applications, impersonate users, escalate privileges and fiddle with #ActiveDirectory permissions in our chal… https://t.co/LuZ1KfthlK\n",
      "The recent @TheJusticeDept framework on cryptocurrency enforcement has taken flak from many in the crypto community https://t.co/sqYm7cJ7mQ\n",
      "RT @LocalCryptosEN: Every #Dash trade so far has used #InstantSend, making trading lightning-fast. ⚡️⚡️⚡️\n",
      "\n",
      "Because LocalCryptos accepts zer…\n",
      "Learn how to perform process injection into a process with various injection techniques. #PentesterAcademy… https://t.co/a7u8c24Nla\n",
      "This week, #Bitcoin stays resilient, while @ethereum &amp; @AaveAave prepare for upgrades! \n",
      "\n",
      "A weekly data perspective… https://t.co/8K7XQwO1hS\n",
      "In most places you can get a RON, now it's time for AI. #ArtificialIntelligence #MachineLearning #aiethics https://t.co/0jX4D8DOWH\n",
      "#Binance Extends Zero Maker Fee Promotion for $BUSD to 2021/02/08\n",
      "\n",
      "https://t.co/i8iRoETBEW\n",
      "RT @axios: NEW: House Speaker Nancy Pelosi and Rep. Jamie Raskin will introduce legislation tomorrow to establish a 25th Amendment commissi…\n",
      "The problem with $ENCORE was that it was launched unfairly and devs loaded up 700ETH worth to dump compared to $DCORE\n",
      "RT @vengist: @ameensol Moloch peace prize\n",
      "AttackDefense Labs | Remote Code Execution: SQL Buddy https://t.co/OS7j1n0ErZ https://t.co/HX0r9bB6XC Learn More About #AttackDefense Today!\n",
      "Instant authorization, best UX, best security needed.\n",
      "RT @NEOnewstoday: In the latest Byzant series article, we talked to @markjeffrey from @guardiancircle about what it takes to succeed when r…\n",
      "RT @FlamingoFinance: This has surpassed $325M, with pnUSDT/pnWBTC being the top pair with most liquidity.\n",
      "RT @coz_official: Check out our latest dora features at https://t.co/Gj6sduJ0gE! https://t.co/FJHTUKh19P\n",
      "RT @john_devadoss: codegen with the Visual Token Designer - demo'ing C# Smart Contracts at the TTF WG https://t.co/426vFAH3Qa\n"
     ]
    }
   ],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition function: Takes 2 arguments; \n",
    "- search = what to search for, \n",
    "- acq = how many tweets to grab\n",
    "\n",
    "> usage: acqData('palantir','100')\n",
    "\n",
    "There’s a lot of metadata in each tweet. I am only extracting ones that I care about. In actuality, it might be better to collect everything and perform post processing after it hits Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acqData(search, acq):\n",
    "\n",
    "    # Create an index name using our search criteria and today's date\n",
    "    index_name = search.split(' ')[0] + '-' + dt.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Initialize the feed list of dictionaries\n",
    "    feed = []\n",
    "    \n",
    "    print('::Acquiring Data::')\n",
    "   \n",
    "    # Data Acquisition\n",
    "    for tweet in tw.Cursor(api.search, q=search, tweet_mode='extended').items(acq):\n",
    "        feed.append(tweet._json)\n",
    "\n",
    "    # Formatting the data and extracting what we need\n",
    "    count = 0\n",
    "    \n",
    "    print('::Transferring to Elasticsearch Search::')\n",
    "    \n",
    "    while count < len(feed):\n",
    "        # Created variables instead of directly injecting it to the dictionary because it's easier to read\n",
    "        tweet_date = feed[count]['created_at']\n",
    "        username = feed[count]['user']['screen_name']\n",
    "        account_creation_date = feed[count]['created_at']\n",
    "        user_description = feed[count]['user']['description']\n",
    "        user_url = feed[count]['user']['url']\n",
    "        verified_status = feed[count]['user']['verified']\n",
    "        geo_enabled = feed[count]['user']['geo_enabled']\n",
    "        friends_count = feed[count]['user']['friends_count']\n",
    "        followers_count = feed[count]['user']['followers_count']\n",
    "        retweeted_count = feed[count]['retweet_count']\n",
    "        favorite_count = feed[count]['favorite_count']\n",
    "        hashtags = feed[count]['entities']['hashtags']\n",
    "        tweet_full_text = feed[count]['full_text']\n",
    "\n",
    "        # Prepare data for elasticsearch\n",
    "        doc = {\n",
    "            '@timestamp': dt.now(),\n",
    "            'tweet_date': tweet_date,\n",
    "            'username': str(username),\n",
    "            'account_creation_date': str(account_creation_date),\n",
    "            'user_description': str(user_description),\n",
    "            'user_url': str(user_url),\n",
    "            'verified_status': bool(verified_status),\n",
    "            'geo_enabled': bool(geo_enabled),\n",
    "            'friends_count': int(friends_count),\n",
    "            'followers_count': int(followers_count),\n",
    "            'retweeted_count': int(retweeted_count),\n",
    "            'favorite_count': int(favorite_count),\n",
    "            'hashtags': hashtags,\n",
    "            'tweet_full_text': str(tweet_full_text),\n",
    "            'word_list': str(tweet_full_text).split(' ')\n",
    "        }\n",
    "\n",
    "        # Import into elasticsearch using the generated index name at the top of the function <search> + <date>\n",
    "        es.index(index=index_name, body=doc)\n",
    "\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we are calling the acqData function and passing the arguments ‘palantir OR PLTR’ and 100. I am looking for any keyword matches for “palantir OR PLTR” and I want to grab 100 most recent tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Acquiring Data::\n",
      "::Transferring to Elasticsearch Search::\n"
     ]
    }
   ],
   "source": [
    "# Main Function; Let's run this function!\n",
    "acqData('palantir OR PLTR', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AcqData Sample Image](images/acqData_sample1.png)\n",
    "\n",
    "We now have the data in the format we want and it imported into Elasticsearch using the es.index method in our acqData function. Let’s check Kibana.\n",
    "\n",
    "![Screenshot of data imported into Elasticsearch](images/es_stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts:\n",
    "Part 1 of this series focuses on data acquisition and using python along with 2 modules: Tweepy and Elasticsearch. We used Tweepy as programmatic interface to Twitter and then used Elasticsearch to inject tweets into the database. Looking at Kibana, it already has some really useful information:\n",
    "\n",
    "- Percentage of usernames that tweeted about the topic in the 2500 tweet download\n",
    "- Percentage of retweets found in the 2500 tweet download\n",
    "\n",
    "In Part 2, we’ll explore the data a little bit more and try to answer some of the questions that we initially came up with. We’ll create dashboards and visualizations and explore what else we can extract from the information we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
